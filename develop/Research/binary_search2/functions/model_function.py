# model_function.py
# ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ä½¿ç”¨ã—ãŸç•°å¸¸æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã®ã‚³ã‚¢æ©Ÿèƒ½

# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import numpy as np
import pandas as pd
import random
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, initializers
from keras.saving import register_keras_serializable

import os
import multiprocessing
from functools import partial

from custom_Class.custom_earlystopping import CustomAutoencoder, MaxReconstructionErrorEarlyStopping
import config


# =============================================================================
# ã‚·ãƒ¼ãƒ‰å€¤é–¢é€£
# =============================================================================

def set_seed(seed):
    """
    å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰å€¤è¨­å®š
    
    å…¥åŠ›:
        seed: int - ã‚·ãƒ¼ãƒ‰å€¤
    å‡ºåŠ›:
        ãªã—
    """
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)


def generate_seeds(seed_start, num_retry):
    """
    ã‚·ãƒ¼ãƒ‰å€¤ãƒªã‚¹ãƒˆã‚’è‡ªå‹•ç”Ÿæˆ
    
    å…¥åŠ›:
        seed_start: int - ã‚·ãƒ¼ãƒ‰å€¤ã®æœ€åˆã®å€¤
        num_retry: int - åˆæœŸå€¤æŒ¯ã‚Šç›´ã—å›æ•°
    å‡ºåŠ›:
        seeds: list - ã‚·ãƒ¼ãƒ‰å€¤ãƒªã‚¹ãƒˆ
    """
    return [seed_start + i for i in range(num_retry)]


# =============================================================================
# åŸºæº–å€¤è¨ˆç®—é–¢é€£
# =============================================================================

def calculate_significant_digits(value):
    """
    æ•°å€¤ã®æœ‰åŠ¹æ¡æ•°ã‚’è¨ˆç®—ï¼ˆæ•´æ•°éƒ¨ + å°æ•°éƒ¨ï¼‰
    
    å…¥åŠ›:
        value: float - è¨ˆç®—å¯¾è±¡ã®å€¤
    å‡ºåŠ›:
        significant_digits: int - æœ‰åŠ¹æ¡æ•°
    """
    if value == 0:
        return 1
    
    abs_value = abs(value)
    
    # æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦æ¡æ•°ã‚’æ•°ãˆã‚‹
    value_str = str(abs_value)
    
    # å°æ•°ç‚¹ã‚’é™¤å»ã—ã¦æ¡æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    if '.' in value_str:
        # å°æ•°ç‚¹ã‚’é™¤ã„ãŸæ•°å­—ã®æ¡æ•°
        significant_digits = len(value_str) - 1  # å°æ•°ç‚¹ã®1æ–‡å­—åˆ†ã‚’å¼•ã
    else:
        # æ•´æ•°ã®å ´åˆ
        significant_digits = len(value_str)
    
    return significant_digits


def calculate_thresholds(train_data_original, columns_list):
    """
    å„ç‰¹å¾´é‡ã®åŸºæº–å€¤ã‚’è¨ˆç®—
    
    å…¥åŠ›:
        train_data_original: ndarray - æ­£è¦åŒ–å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        columns_list: list - ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
    å‡ºåŠ›:
        thresholds: dict - ç‰¹å¾´é‡åâ†’åŸºæº–å€¤ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    """
    thresholds = {}
    
    for i, col_name in enumerate(columns_list):
        # å„ç‰¹å¾´é‡ã®æœ€å¤§å€¤ã‚’å–å¾—
        max_value = np.max(np.abs(train_data_original[:, i]))
        
        # æœ‰åŠ¹æ¡æ•°ã‚’è¨ˆç®—
        significant_digits = calculate_significant_digits(max_value)
        
        # åŸºæº–å€¤ = 10^(-æœ‰åŠ¹æ¡æ•°)
        threshold = 10 ** (-significant_digits)
        thresholds[col_name] = threshold

        # ãƒ‡ãƒãƒƒã‚°ç”¨å‡ºåŠ›
        print(f"ç‰¹å¾´é‡: {col_name}, æœ€å¤§å€¤: {max_value}, æœ‰åŠ¹æ¡æ•°: {significant_digits}, åŸºæº–å€¤: {threshold}")
    
    return thresholds

def generate_test_thresholds(columns_list, test_threshold):
    """
    ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ã®å›ºå®šåŸºæº–å€¤ã‚’ç”Ÿæˆ
    
    å…¥åŠ›:
        columns_list: list - ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        test_threshold: float - å…¨ç‰¹å¾´å…±é€šã®åŸºæº–å€¤
    å‡ºåŠ›:
        thresholds: dict - ç‰¹å¾´é‡åâ†’åŸºæº–å€¤ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    """
    thresholds = {}
    for col_name in columns_list:
        thresholds[col_name] = test_threshold
    print(f"[ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰] å…¨ç‰¹å¾´ã®åŸºæº–å€¤ã‚’ {test_threshold} ã«è¨­å®š")
    return thresholds
# =============================================================================
# èª¤å·®è¨ˆç®—é–¢é€£
# =============================================================================

@register_keras_serializable()
def root_mean_squared_error(y_true, y_pred):
    """
    å…¨ä½“ã®RMSEè¨ˆç®—ï¼ˆæå¤±é–¢æ•°ç”¨ï¼‰
    
    å…¥åŠ›:
        y_true: Tensor - æ­£è§£ãƒ‡ãƒ¼ã‚¿
        y_pred: Tensor - äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
    å‡ºåŠ›:
        rmse: Tensor - RMSEå€¤ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ï¼‰
    """
    mse_total = tf.reduce_mean(tf.square(y_pred - y_true))
    rmse_total = tf.sqrt(mse_total)
    return rmse_total


def root_mean_squared_error_per_data(y_true, y_pred):
    """
    ãƒ‡ãƒ¼ã‚¿ã”ã¨ã®RMSEè¨ˆç®—
    
    å…¥åŠ›:
        y_true: Tensor - æ­£è§£ãƒ‡ãƒ¼ã‚¿
        y_pred: Tensor - äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
    å‡ºåŠ›:
        rmse_per_data: Tensor - å„ãƒ‡ãƒ¼ã‚¿ã®RMSEï¼ˆå½¢çŠ¶: [N, 1]ï¼‰
    """
    mse_per_sample = tf.reduce_mean(tf.square(y_pred - y_true), axis=1)
    rmse_per_sample = tf.sqrt(mse_per_sample)
    errors = tf.reshape(rmse_per_sample, (-1, 1))
    return errors


def root_mean_squared_error_per_feature(y_true, y_pred):
    """
    ç‰¹å¾´é‡ã”ã¨ã®RMSEè¨ˆç®—
    
    å…¥åŠ›:
        y_true: ndarray - æ­£è§£ãƒ‡ãƒ¼ã‚¿
        y_pred: ndarray - äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
    å‡ºåŠ›:
        rmse_per_feature: ndarray - å„ç‰¹å¾´é‡ã®RMSEï¼ˆå½¢çŠ¶: [ç‰¹å¾´æ•°]ï¼‰
    """
    # å„ç‰¹å¾´é‡ã”ã¨ã®èª¤å·®ã‚’è¨ˆç®—
    errors = np.sqrt((y_true - y_pred) ** 2)
    # å„ç‰¹å¾´é‡ã”ã¨ã«å¹³å‡ã‚’å–ã‚‹
    mean_errors_per_feature = np.mean(errors, axis=0)
    return mean_errors_per_feature


def root_mean_squared_error_per_feature_per_data(y_true, y_pred):
    """
    å„ãƒ‡ãƒ¼ã‚¿ã®å„ç‰¹å¾´é‡ã”ã¨ã®RMSEè¨ˆç®—
    
    å…¥åŠ›:
        y_true: ndarray - æ­£è§£ãƒ‡ãƒ¼ã‚¿
        y_pred: ndarray - äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
    å‡ºåŠ›:
        errors: ndarray - å„ãƒ‡ãƒ¼ã‚¿ã®å„ç‰¹å¾´é‡ã®RMSEï¼ˆå½¢çŠ¶: [N, ç‰¹å¾´æ•°]ï¼‰
    """
    errors = np.sqrt((y_true - y_pred) ** 2)
    return errors


# =============================================================================
# ãƒ‡ãƒ¼ã‚¿è¨ˆç®—é–¢é€£
# =============================================================================

def calculate_data(train_data, test_data, train_predict_data, test_predict_data, flag_skip=0):
    """
    å„ç¨®èª¤å·®ãƒ»ç•°å¸¸ã‚¹ã‚³ã‚¢ã‚’ä¸€æ‹¬è¨ˆç®—
    
    å…¥åŠ›:
        train_data: ndarray - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        test_data: ndarray - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        train_predict_data: ndarray - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
        test_predict_data: ndarray - ãƒ†ã‚¹ãƒˆäºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
        flag_skip: int - ã‚¹ã‚­ãƒƒãƒ—ãƒ•ãƒ©ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0ï¼‰
    å‡ºåŠ›ï¼ˆflag_skip=0ã®å ´åˆï¼‰:
        abnormal_score: ndarray - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸ã‚¹ã‚³ã‚¢
        threshold: float - é–¾å€¤
        errors_train_per_features_avg: Series - ç‰¹å¾´é‡ã”ã¨ã®å¹³å‡å†æ§‹æˆèª¤å·®
        errors_predict_per_features_avg: Series - ãƒ†ã‚¹ãƒˆã®ç‰¹å¾´é‡ã”ã¨ã®å¹³å‡å†æ§‹æˆèª¤å·®
        threshold_per_features: Series - é–¾å€¤ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã”ã¨å†æ§‹æˆèª¤å·®
        reconstruct_error: float - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®å†æ§‹æˆèª¤å·®
        errors_train_per_data: Tensor - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã”ã¨ã®å†æ§‹æˆèª¤å·®
    å‡ºåŠ›ï¼ˆflag_skip=1ã®å ´åˆï¼‰:
        abnormal_score: ndarray - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸ã‚¹ã‚³ã‚¢
        errors_predict_per_features_avg: Series - ãƒ†ã‚¹ãƒˆã®ç‰¹å¾´é‡ã”ã¨ã®å¹³å‡å†æ§‹æˆèª¤å·®
    """
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®è¨ˆç®—
    errors_train = root_mean_squared_error_per_data(train_data, train_predict_data)
    errors_predict = root_mean_squared_error_per_data(test_data, test_predict_data)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸ã‚¹ã‚³ã‚¢
    abnormal_score = errors_predict.numpy().flatten()
    
    # ç‰¹å¾´é‡ã”ã¨ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹å†æ§‹æˆèª¤å·®ã®å¹³å‡
    errors_predict_per_features = root_mean_squared_error_per_feature(test_data, test_predict_data)
    errors_predict_per_features_avg = pd.Series(errors_predict_per_features, index=config.columns_list)
    
    if flag_skip == 1:
        return abnormal_score, errors_predict_per_features_avg
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹å†æ§‹æˆèª¤å·®ã®è¨ˆç®—
    reconstruct_error = root_mean_squared_error(train_data, train_predict_data).numpy()
    
    # å„ãƒ‡ãƒ¼ã‚¿ã”ã¨ã®å†æ§‹æˆèª¤å·®
    errors_train_per_data = errors_train
    
    # ç‰¹å¾´é‡ã”ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹å†æ§‹æˆèª¤å·®
    errors_train_per_features = root_mean_squared_error_per_feature(train_data, train_predict_data)
    errors_train_per_features_avg = pd.Series(errors_train_per_features, index=config.columns_list)
    
    # å„ãƒ‡ãƒ¼ã‚¿ã€å„ç‰¹å¾´é‡ã®å†æ§‹æˆèª¤å·®
    errors_per_data_per_features = root_mean_squared_error_per_feature_per_data(train_data, train_predict_data)
    errors_per_data_per_features = pd.DataFrame(errors_per_data_per_features, columns=config.columns_list)
    
    # é–¾å€¤ã®è¨ˆç®—ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§å†æ§‹æˆèª¤å·®ï¼‰
    threshold = errors_train.numpy().max()
    
    # é–¾å€¤ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã”ã¨ã®å†æ§‹æˆèª¤å·®
    max_position = int(tf.argmax(errors_train).numpy())
    print(f"æœ€ã‚‚å†æ§‹æˆèª¤å·®ãŒå¤§ãã‹ã£ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ï¼š{max_position}")
    threshold_per_features = errors_per_data_per_features.iloc[max_position]
    
    return (abnormal_score, threshold, errors_train_per_features_avg, 
            errors_predict_per_features_avg, threshold_per_features, 
            reconstruct_error, errors_train_per_data)


# =============================================================================
# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰é–¢é€£
# =============================================================================

def model_autoencoder(params, seed, unit13, unit2):
    """
    3å±¤ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    
    å…¥åŠ›:
        params: dict - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        seed: int - ã‚·ãƒ¼ãƒ‰å€¤
        unit13: int - ä¸­é–“å±¤1, 3ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        unit2: int - ä¸­é–“å±¤2ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
    å‡ºåŠ›:
        model: CustomAutoencoder - ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    """
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
    learning_rate = params["learning_rate"]
    input_unit = params["unit"]
    
    # ã‚·ãƒ¼ãƒ‰å€¤ã®ã‚»ãƒƒãƒˆ
    set_seed(seed)
    
    # ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã®å®šç¾©
    middle_unit_1 = unit13
    middle_unit_2 = unit2
    middle_unit_3 = unit13
    output_unit = input_unit
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®å®šç¾©ï¼ˆFunctional APIï¼‰
    inputs = keras.Input(shape=(input_unit,))
    
    x = layers.Dense(
        units=middle_unit_1, 
        activation="sigmoid",
        kernel_initializer=initializers.TruncatedNormal(
            mean=0.0, stddev=pow(unit13, -0.5), seed=seed
        )
    )(inputs)
    
    x = layers.Dense(
        units=middle_unit_2, 
        activation="sigmoid",
        kernel_initializer=initializers.TruncatedNormal(
            mean=0.0, stddev=pow(unit2, -0.5), seed=seed
        )
    )(x)
    
    x = layers.Dense(
        units=middle_unit_3, 
        activation="sigmoid",
        kernel_initializer=initializers.TruncatedNormal(
            mean=0.0, stddev=pow(unit13, -0.5), seed=seed
        )
    )(x)
    
    outputs = layers.Dense(
        units=output_unit, 
        activation="linear",
        kernel_initializer=initializers.TruncatedNormal(
            mean=0.0, stddev=pow(175, -0.5), seed=seed
        )
    )(x)
    
    model = CustomAutoencoder(inputs, outputs)
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss=root_mean_squared_error)
    
    return model


# =============================================================================
# ãƒ­ã‚°é–¢é€£
# =============================================================================

def initialize_log_directories(base_path, seeds, period_log):
    """
    å­¦ç¿’ãƒ­ã‚°ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆæœŸåŒ–
    
    å…¥åŠ›:
        base_path: str - ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        seeds: list - ã‚·ãƒ¼ãƒ‰å€¤ãƒªã‚¹ãƒˆ
        period_log: int - æœŸé–“ç•ªå·
    å‡ºåŠ›:
        ãªã—
    """
    for seed in seeds:
        dir_path = os.path.join(
            base_path, 
            f"learning_scores_seed{seed}",
            f"learning_scores_seed{seed}_period{period_log}"
        )
        os.makedirs(dir_path, exist_ok=True)


def save_learning_scores(log_data, base_path, seed, period_log, unit13, unit2):
    """
    å­¦ç¿’ãƒ­ã‚°ã‚’CSVã«ä¿å­˜
    
    å…¥åŠ›:
        log_data: dict - ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®å­¦ç¿’ãƒ­ã‚°
        base_path: str - ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        seed: int - ã‚·ãƒ¼ãƒ‰å€¤
        period_log: int - æœŸé–“ç•ªå·
        unit13: int - ä¸­é–“å±¤1, 3ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        unit2: int - ä¸­é–“å±¤2ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
    å‡ºåŠ›:
        ãªã—
    """
    if not log_data or len(log_data["epoch"]) == 0:
        return
    
    # DataFrameã«å¤‰æ›
    df = pd.DataFrame(log_data)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ç”Ÿæˆ
    dir_path = os.path.join(
        base_path,
        f"learning_scores_seed{seed}",
        f"learning_scores_seed{seed}_period{period_log}"
    )
    file_name = f"learning_scores_seed{seed}_period{period_log}_unit13_{unit13}_unit2_{unit2}.csv"
    file_path = os.path.join(dir_path, file_name)
    
    # ä¿å­˜
    df.to_csv(file_path, index=False)


# =============================================================================
# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–¢é€£
# =============================================================================

def learn_model(params, model, train_data, thresholds, seed, period_log, unit13, unit2, base_path):
    """
    ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè¡Œ
    
    å…¥åŠ›:
        params: dict - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        model: CustomAutoencoder - ãƒ¢ãƒ‡ãƒ«
        train_data: ndarray - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        thresholds: dict - ç‰¹å¾´é‡ã”ã¨ã®åŸºæº–å€¤
        seed: int - ã‚·ãƒ¼ãƒ‰å€¤
        period_log: int - æœŸé–“ç•ªå·
        unit13: int - ä¸­é–“å±¤1, 3ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        unit2: int - ä¸­é–“å±¤2ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        base_path: str - ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
    å‡ºåŠ›:
        model: CustomAutoencoder - å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    """
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã°ã‚‰ã™
    batch_size = params["batch_size"]
    max_epochs = params["max_epochs"]
    log_counter = params["log_counter"]
    detail_log = params["detail_log"]
    
    # early_stoppingã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    early_stopping_params = {
        "detail_log": detail_log,
        "thresholds": thresholds,
        "log_counter": log_counter
    }
    
    # early_stoppingã®è¨­å®š
    early_stopping = MaxReconstructionErrorEarlyStopping(
        model, early_stopping_params,
        seed=seed, period_log=period_log, unit13=unit13, unit2=unit2, base_path=base_path
    )
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™
    model.train_data_for_monitoring = train_data
    
    # å­¦ç¿’é€Ÿåº¦é«˜é€ŸåŒ–ã®ãŸã‚tfå½¢å¼ã«å¤‰æ›´
    train_data_tf = tf.cast(train_data, dtype=tf.float32)
    
    # å­¦ç¿’ã®é–‹å§‹
    history = model.fit(
        train_data_tf, train_data_tf, 
        epochs=max_epochs, 
        batch_size=batch_size, 
        verbose=0,
        shuffle=True, 
        validation_data=(train_data_tf, train_data_tf),
        callbacks=[early_stopping]
    )
    
    return model


# =============================================================================
# ä¸¦åˆ—å‡¦ç†é–¢é€£
# =============================================================================

def run_one_seed(params, train_data, unit13, unit2, period_log, thresholds, base_path, init_num):
    """
    1ã¤ã®ã‚·ãƒ¼ãƒ‰å€¤ã§ã®å­¦ç¿’å®Ÿè¡Œï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
    
    å…¥åŠ›:
        params: dict - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        train_data: ndarray - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        unit13: int - ä¸­é–“å±¤1, 3ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        unit2: int - ä¸­é–“å±¤2ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        period_log: int - æœŸé–“ç•ªå·
        thresholds: dict - ç‰¹å¾´é‡ã”ã¨ã®åŸºæº–å€¤
        base_path: str - ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        init_num: int - åˆæœŸåŒ–ç•ªå·ï¼ˆ0ã€œnum_retry-1ï¼‰
    å‡ºåŠ›:
        model: CustomAutoencoder - å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        success: bool - åŸºæº–å€¤ã‚’æº€ãŸã—ãŸã‹
        seed: int - ä½¿ç”¨ã—ãŸã‚·ãƒ¼ãƒ‰å€¤
    """
    # seedã®å–ã‚Šå‡ºã—
    seeds = generate_seeds(params["seed_start"], params["num_retry"])
    seed = seeds[init_num]
    set_seed(seed)
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    model = model_autoencoder(params, seed, unit13, unit2)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model = learn_model(
        params, model, train_data, thresholds,
        seed, period_log, unit13, unit2, base_path
    )
    
    # æˆåŠŸåˆ¤å®š
    if hasattr(model, 'reached_threshold') and model.reached_threshold:
        return model, True, seed
    else:
        return model, False, seed


def try_init_point(params, train_data, unit13, unit2, period_log, thresholds, base_path):
    """
    è¤‡æ•°ã‚·ãƒ¼ãƒ‰ã§ã®ä¸¦åˆ—å­¦ç¿’ãƒ»æˆåŠŸåˆ¤å®š
    
    å…¥åŠ›:
        params: dict - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        train_data: ndarray - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        unit13: int - ä¸­é–“å±¤1, 3ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        unit2: int - ä¸­é–“å±¤2ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        period_log: int - æœŸé–“ç•ªå·
        thresholds: dict - ç‰¹å¾´é‡ã”ã¨ã®åŸºæº–å€¤
        base_path: str - ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
    å‡ºåŠ›:
        model: CustomAutoencoder - æˆåŠŸã—ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆã¾ãŸã¯æœ€å¾Œã®ãƒ¢ãƒ‡ãƒ«ï¼‰
        success: bool - åŸºæº–å€¤ã‚’æº€ãŸã—ãŸã‹
    """
    num_retry = params["num_retry"]
    
    print(f"å­¦ç¿’æœŸé–“: {period_log}")
    
    # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã§ã®å®Ÿè¡Œ
    with multiprocessing.Pool(processes=num_retry) as pool:
        result_iter = pool.imap_unordered(
            partial(
                run_one_seed, params, train_data, unit13, unit2, 
                period_log, thresholds, base_path
            ),
            list(range(num_retry))
        )
        
        success_found = False
        last_model = None
        
        for model, success, seed in result_iter:
            last_model = model
            if success:
                print(f"âœ… æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ (seed={seed})")
                success_found = True
                break
            else:
                print(f"é–¾å€¤ã‚’ä¸‹å›ã‚Šã¾ã›ã‚“ã§ã—ãŸ (seed={seed})")
        
        pool.close()
        pool.join()
        
        if success_found:
            return model, True
        else:
            print(f"å…¨ã¦ã®åˆæœŸå€¤ã§é–¾å€¤ã‚’ä¸‹å›ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (unit13={unit13}, unit2={unit2})")
            print("----------------------------------------------------------------------------------------------------------------")
            return last_model, False


# =============================================================================
# ãƒãƒ¼ãƒ‰æ¢ç´¢é–¢é€£
# =============================================================================

def search_unit13_recursive(params, train_data, unit2, period_log, lower_bound, upper_bound, 
                            best_model, best_unit13, min_total_units, thresholds, base_path, searched_unit13):
    """
    unit13ã®å†å¸°çš„äºŒåˆ†æ¢ç´¢
    
    å…¥åŠ›:
        params: dict - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        train_data: ndarray - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        unit2: int - ä¸­é–“å±¤2ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ï¼ˆå›ºå®šï¼‰
        period_log: int - æœŸé–“ç•ªå·
        lower_bound: int - æ¢ç´¢ä¸‹é™
        upper_bound: int - æ¢ç´¢ä¸Šé™
        best_model: CustomAutoencoder - ç¾åœ¨ã®ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
        best_unit13: int - ç¾åœ¨ã®ãƒ™ã‚¹ãƒˆunit13
        min_total_units: int - ç¾åœ¨ã®æœ€å°åˆè¨ˆãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        thresholds: dict - ç‰¹å¾´é‡ã”ã¨ã®åŸºæº–å€¤
        base_path: str - ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
    å‡ºåŠ›:
        best_model: CustomAutoencoder - ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
        best_unit13: int - ãƒ™ã‚¹ãƒˆunit13
        min_total_units: int - æœ€å°åˆè¨ˆãƒ¦ãƒ‹ãƒƒãƒˆæ•°
    """
    # å¤‰æ›´å¾Œï¼ˆæ‰“ã¡åˆ‡ã‚Šæ¡ä»¶ï¼‰
    mid_unit13 = (lower_bound + upper_bound) // 2

    if mid_unit13 in searched_unit13:
        print(f"[å†å¸°æ¢ç´¢çµ‚äº†] unit13={mid_unit13} ã¯æ¢ç´¢æ¸ˆã¿ â†’ æ‰“ã¡åˆ‡ã‚Š")
        return best_model, best_unit13, min_total_units

    searched_unit13.add(mid_unit13)
    
    print("----------------------------------------------------------------------------------------------------------------")
    print(f"[å†å¸°æ¢ç´¢] unit13={mid_unit13}, unit2={unit2}")
    
    model, flag_low_threshold = try_init_point(
        params, train_data, mid_unit13, unit2, period_log, thresholds, base_path
    )
    total_units = mid_unit13 * 2 + unit2
    
    if flag_low_threshold:
        print(f"âœ… é–¾å€¤ã‚’ä¸‹å›ã‚Šã¾ã—ãŸ: unit13={mid_unit13}")
        if total_units < min_total_units:
            best_model = model
            best_unit13 = mid_unit13
            min_total_units = total_units
        
        # å†å¸°çš„ã«ä¸‹é™ã‚’æ›´æ–°
        return search_unit13_recursive(
            params, train_data, unit2, period_log,
            lower_bound, mid_unit13, best_model, best_unit13, min_total_units,
            thresholds, base_path, searched_unit13
        )
    else:
        # å†å¸°çš„ã«ä¸Šé™ã‚’æ›´æ–°
        return search_unit13_recursive(
            params, train_data, unit2, period_log,
            mid_unit13, upper_bound, best_model, best_unit13, min_total_units,
            thresholds, base_path, searched_unit13
        )


def search_optimal_units(params, train_data, period_log, thresholds, base_path):
    """
    ãƒ¡ã‚¤ãƒ³ãƒãƒ¼ãƒ‰æ¢ç´¢ãƒ­ã‚¸ãƒƒã‚¯
    
    å…¥åŠ›:
        params: dict - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        train_data: ndarray - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        period_log: int - æœŸé–“ç•ªå·
        thresholds: dict - ç‰¹å¾´é‡ã”ã¨ã®åŸºæº–å€¤
        base_path: str - ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
    å‡ºåŠ›:
        best_model: CustomAutoencoder - æœ€é©ãƒ¢ãƒ‡ãƒ«
    """
    # åˆæœŸå€¤ã®è¨­å®š
    best_model = None
    best_unit_1_3 = None
    best_unit_2 = None
    min_total_units = float("inf")
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
    units = params["unit"]
    first_unit13 = units * 2
    max_unit2 = units - 1
    
    # ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã‚’ä¿æŒã™ã‚‹å¤‰æ•°
    previous_best_unit13 = None
    previous_unit2 = None
    
    # çµ‚ã‚ã‚Šã®ãƒ•ãƒ©ã‚°
    end_flag = False
    
    # æ¢ç´¢é–‹å§‹
    for unit_2 in reversed(range(1, max_unit2 + 1)):
        print(f"\n===== unit2={unit_2} ã®æ¢ç´¢é–‹å§‹ =====")
        
        if previous_best_unit13 is None:
            unit13 = first_unit13
        else:
            # å°æ•°åˆ‡ã‚Šæ¨ã¦
            unit13 = (previous_best_unit13 * 2 + previous_unit2 - unit_2) // 2
        
        print(f"unit2={unit_2} ã«å¯¾ã™ã‚‹unit13æ¢ç´¢é–‹å§‹ç‚¹: {unit13}")
        
        # æ¢ç´¢ã—ãŸãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã‚’è¨˜éŒ²ã™ã‚‹ã‚»ãƒƒãƒˆ
        searched_unit13 = set()
        
        # ãƒãƒ¼ãƒ‰æ¢ç´¢ã«ä½¿ç”¨ã™ã‚‹å¤‰æ•°
        lower = units + 1
        upper = None
        
        # å„unit13ã«å¯¾ã™ã‚‹upperã‚’å®šç¾©ã™ã‚‹
        if unit_2 == max_unit2:
            unit13 = first_unit13
            while True:
                print("----------------------------------------------------------------------")
                print(f"[unit{max_unit2}æ¢ç´¢] unit13={unit13}, unit2={unit_2}, lower={lower}, upper={upper}")
                
                searched_unit13.add(unit13)
                
                # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
                model, flag_low_threshold = try_init_point(
                    params, train_data, unit13, unit_2, period_log, thresholds, base_path
                )
                
                if flag_low_threshold:
                    # upperã®ä»£å…¥
                    upper = unit13
                    total_units = unit13 * 2 + unit_2
                    min_total_units = total_units
                    best_model = model
                    best_unit_1_3 = unit13
                    best_unit_2 = unit_2
                    print(f"ğŸ‰ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«æ›´æ–°: unit13={unit13}, unit2={unit_2}, åˆè¨ˆ={total_units}")
                    unit13 = (upper + lower) // 2
                    break
                
                lower = unit13
                unit13 *= 2  # è¦‹ã¤ã‹ã‚‹ã¾ã§å€ã€…æ¢ç´¢
        
        else:
            # unit2 < max_unit2 ã®ã¨ãï¼šupper ã¯éå»ã®ãƒ™ã‚¹ãƒˆæ§‹æˆã‹ã‚‰è¨ˆç®—
            upper = ((best_unit_1_3 * 2 + best_unit_2) - unit_2) // 2
            unit13 = upper
        
        # unit13ã®æ¢ç´¢
        while True:
            # åæŸã™ã‚‹ã‹ã€å…¥åŠ›ãƒãƒ¼ãƒ‰æ•°ã‚’ä¸‹å›ã£ãŸã‚‰çµ‚äº†
            if unit13 in searched_unit13 or unit13 <= units:
                print(f"æ¢ç´¢ãŒåæŸ or ç¯„å›²å¤–ï¼ˆunit13={unit13}ï¼‰ã€‚unit2={unit_2} ã®æ¢ç´¢çµ‚äº†ã€‚")
                break
            
            searched_unit13.add(unit13)
            print(f"[æ¢ç´¢ã®é–‹å§‹] unit13={unit13}, unit2={unit_2}")
            
            # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
            model, flag_low_threshold = try_init_point(
                params, train_data, unit13, unit_2, period_log, thresholds, base_path
            )
            
            if not flag_low_threshold and len(searched_unit13) == 1 and unit_2 != max_unit2:
                # åˆå›ã®æ¢ç´¢ã§é–¾å€¤ã‚’ä¸‹å›ã‚‰ãªã‹ã£ãŸå ´åˆã€æ¢ç´¢ã‚’çµ‚äº†
                print(f"unit2={unit_2} ã«å¯¾ã™ã‚‹unit13={unit13} ã§ã‚‚é–¾å€¤ã‚’ä¸‹å›ã‚‰ãªã‹ã£ãŸã®ã§ä»¥é™ã®æ¢ç´¢ã‚’å®Œå…¨ã«çµ‚äº†ã—ã¾ã™")
                
                if best_model is None:
                    print("â— æœ€é©ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœ€å¾Œã«æ¢ç´¢ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã—ã¾ã™ã€‚")
                    return model
                
                end_flag = True
                break
            
            # é–¾å€¤ã‚’ä¸‹å›ã£ãŸå ´åˆã€æœ€é©ãªãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã‚’æ›´æ–°
            if flag_low_threshold:
                total_units = unit13 * 2 + unit_2
                if total_units < min_total_units:
                    min_total_units = total_units
                    best_model = model
                    best_unit_1_3 = unit13
                    best_unit_2 = unit_2
                    print(f"ğŸ‰ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«æ›´æ–°: unit13={unit13}, unit2={unit_2}, åˆè¨ˆ={total_units}")
                    print("------------------------------------------------------------------------------------------")
                upper = unit13
                
                # æ¢ç´¢ã®å†å¸°çš„ãªå‘¼ã³å‡ºã—
                model, best_unit1_3, min_total_units = search_unit13_recursive(
                    params, train_data, unit_2, period_log,
                    lower, upper, best_model, unit13, min_total_units,
                    thresholds, base_path, searched_unit13
                )
                best_model = model
                best_unit_1_3 = best_unit1_3
                best_unit_2 = unit_2
                print("===========================================================================")
                print(f"âœ… unit2={unit_2} ã«å¯¾ã—ã¦æ±ºå®šã•ã‚ŒãŸ unit13={best_unit1_3}")
                break
            
            else:
                lower = unit13
                
                model, best_unit1_3, min_total_units = search_unit13_recursive(
                    params, train_data, unit_2, period_log,
                    lower, upper, best_model, upper, min_total_units,
                    thresholds, base_path, searched_unit13
                )
                best_model = model
                best_unit_1_3 = best_unit1_3
                best_unit_2 = unit_2
                print("===========================================================================")
                print(f"âœ… unit2={unit_2} ã«å¯¾ã—ã¦æ±ºå®šã•ã‚ŒãŸ unit13={best_unit1_3}")
                break
        
        # unit2ã§ã®æ¢ç´¢ãŒçµ‚äº†ã—ãŸå ´åˆã€æœ€é©ãªãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã‚’è¨˜éŒ²
        previous_best_unit13 = best_unit_1_3
        previous_unit2 = best_unit_2
        
        if end_flag:
            print("æ¢ç´¢ã‚’çµ‚äº†ã—ã¾ã™")
            break
    
    if best_model is None:
        print("â— æœ€é©ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœ€å¾Œã«æ¢ç´¢ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã—ã¾ã™ã€‚")
        return model
    
    # æœ€é©ãªãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã‚’è¨˜éŒ²
    print("\n===== æœ€é©æ§‹æˆã®å‡ºåŠ› =====")
    print(f"æœ€é©ãƒãƒ¼ãƒ‰æ•°: unit13={best_unit_1_3}, unit2={best_unit_2}")
    print(f"åˆè¨ˆãƒãƒ¼ãƒ‰æ•°: {min_total_units}")
    
    return best_model